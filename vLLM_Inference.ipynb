{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0Giu3IM1mTBQ98SIB89Z1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/llm-finetune/experiment-tracking/blob/main/vLLM_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShoKcoKtILeX"
      },
      "outputs": [],
      "source": [
        "pip install vllm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# choosing the large language model\n",
        "#llm = LLM(model=\"gpt2-xl\")\n",
        "llm = LLM(model=\"llm-finetune/gemma-2b-alpaca-full\")\n",
        "# setting the parameters\n",
        "sampling_params = SamplingParams(temperature=0.8, top_p=0.90,max_tokens = 50)"
      ],
      "metadata": {
        "id": "P3Tw6Jp3IUKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# defining our prompt\n",
        "prompt = \"Machine Learning is\"\n",
        "\n",
        "# generating the answer\n",
        "answer = llm.generate(prompt,sampling_params)\n",
        "\n",
        "# getting the generated text out from the answer variable\n",
        "answer[0].outputs[0].text"
      ],
      "metadata": {
        "id": "_GZDieEcKn27",
        "outputId": "712face7-c66d-4729-b119-5646d9176fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 942 ms, sys: 8.92 ms, total: 951 ms\n",
            "Wall time: 1 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' a method of calculating mathematical models by applying computational algorithms to large sets of data. Most AI software is built from deep learning algorithms.\\n\\nIn this course, we will start with the basics of deep learning and explain how to implement these algorithms. We'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# defining our prompt\n",
        "prompt = [\n",
        "\n",
        "    \"What is Quantum Computing?\",\n",
        "    \"How are electrons and protons different?\",\n",
        "    \"What is Machine Learning?\",\n",
        "\n",
        "]\n",
        "\n",
        "# generating the answer\n",
        "answers = llm.generate(prompt,sampling_params)\n",
        "\n",
        "# getting the generated text out from the answer variable\n",
        "for i in range(3):\n",
        " print(\"\\nPrompt:\",prompt[i],\"\\nGeneration:\",answers[i].outputs[0].text)\n",
        " print()"
      ],
      "metadata": {
        "id": "5twCR3OcLdR_",
        "outputId": "c9d59b67-5611-495c-a3db-8b78c2a74e93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: What is Quantum Computing? \n",
            "Generation: \n",
            "\n",
            "Quantum computing is a new type of computer built from superconducting circuits. The secret of this is a special property of superconductors: The entire computer can be thought of as a single qubit. This is something that is very\n",
            "\n",
            "\n",
            "Prompt: How are electrons and protons different? \n",
            "Generation: \n",
            "\n",
            "Electrons are the smallest type of particles in our universe. Protons, which are the second smallest, are in the center of the nucleus of atoms. The protons and neutrons in the nucleus are charged particles with an electric field.\n",
            "\n",
            "\n",
            "Prompt: What is Machine Learning? \n",
            "Generation: \n",
            "\n",
            "Machine learning is the study of the way data is processed and how it can be combined to predict future events.\n",
            "\n",
            "It's hard to pin down exactly what Machine Learning is or how it can help you. In fact, a Google search\n",
            "\n",
            "CPU times: user 990 ms, sys: 3.02 ms, total: 993 ms\n",
            "Wall time: 1.01 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8XkqA595LE2p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}